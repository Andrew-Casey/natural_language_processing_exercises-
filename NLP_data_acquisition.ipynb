{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c62db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777e073",
   "metadata": {},
   "source": [
    "Codeup Blog Articles\n",
    "\n",
    "Visit Codeup's Blog and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15c55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the URL from which you want to extract the links\n",
    "url = 'https://codeup.com/blog/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5e98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_all_articles(url):\n",
    "    article_list = []\n",
    "    headers = {\"User-Agent\": \"Chrome/91.0.4472.124\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the href attribute from <a> tags with class 'more-link'\n",
    "    links = soup.find_all('a', class_='more-link')\n",
    "    link_list = [link['href'] for link in links]\n",
    "\n",
    "    for link in link_list:\n",
    "        response = requests.get(link, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title = soup.find('h1', class_='entry-title').text\n",
    "        divcont = soup.find('div', class_='entry-content')\n",
    "        article = [para.text for para in divcont.find_all('p')]\n",
    "\n",
    "        article_dict = {'title': title, 'content': article}\n",
    "        article_list.append(article_dict)\n",
    "\n",
    "    codeup_df = pd.DataFrame(article_list)\n",
    "\n",
    "    return codeup_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a02eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>[May is traditionally known as Asian American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>[Codeup is hosting a Women in Tech Panel in ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Rachel Rob...</td>\n",
       "      <td>[Codeup is hosting a Women in Tech Panel in ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women in Tech: Panelist Spotlight – Sarah Mellor</td>\n",
       "      <td>[Codeup is hosting a Women in Tech Panel in ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women in Tech: Panelist Spotlight – Madeleine ...</td>\n",
       "      <td>[Codeup is hosting a Women in Tech Panel in ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Black Excellence in Tech: Panelist Spotlight –...</td>\n",
       "      <td>[, Codeup is hosting a Black Excellence in Tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1  Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2  Women in tech: Panelist Spotlight – Rachel Rob...   \n",
       "3   Women in Tech: Panelist Spotlight – Sarah Mellor   \n",
       "4  Women in Tech: Panelist Spotlight – Madeleine ...   \n",
       "5  Black Excellence in Tech: Panelist Spotlight –...   \n",
       "\n",
       "                                             content  \n",
       "0  [May is traditionally known as Asian American ...  \n",
       "1  [Codeup is hosting a Women in Tech Panel in ho...  \n",
       "2  [Codeup is hosting a Women in Tech Panel in ho...  \n",
       "3  [Codeup is hosting a Women in Tech Panel in ho...  \n",
       "4  [Codeup is hosting a Women in Tech Panel in ho...  \n",
       "5  [, Codeup is hosting a Black Excellence in Tec...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the main function with the URL\n",
    "codeup_df = get_all_articles(url)\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your link_list with the URLs you want to scrape\n",
    "link_list = [\n",
    "    ['https://example.com/article1', 'https://example.com/article2'],\n",
    "    ['https://example.com/article3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_all_articles(link_list):\n",
    "    article_list = []\n",
    "    headers = {\"User-Agent\": \"Chrome/91.0.4472.124\"}\n",
    "\n",
    "    for links in link_list:\n",
    "        # Iterate over the found links\n",
    "        for link in links:\n",
    "            response = requests.get(link, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract the href attribute from <a> tags with class 'more-link'\n",
    "            links = soup.find_all('a', class_='more-link')\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                print(href)  # Output or further process the href value\n",
    "\n",
    "            title = soup.find('h1', class_='entry-title').text\n",
    "            divcont = soup.find('div', class_='entry-content')\n",
    "            article = [para.text for para in divcont.find_all('p')]\n",
    "\n",
    "            article_dict = {'title': title, 'content': article}\n",
    "            article_list.append(article_dict)\n",
    "\n",
    "    codeup_df = pd.DataFrame(article_list)\n",
    "\n",
    "    return codeup_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ecad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the main function with the link_list\n",
    "codeup_df = get_all_articles(link_list)\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37ba8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_all_articles(link_list):\n",
    "    article_list = []\n",
    "    headers = {\"User-Agent\": \"Chrome/91.0.4472.124\"}\n",
    "\n",
    "    for links in link_list:\n",
    "        # Iterate over the found links\n",
    "        for link in links:\n",
    "            response = requests.get(link, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract the href attribute from <a> tags with class 'more-link'\n",
    "            links = soup.find_all('a', class_='more-link')\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                print(href)  # Output or further process the href value\n",
    "\n",
    "            title = soup.find('h1', class_='entry-title').text\n",
    "            divcont = soup.find('div', class_='entry-content')\n",
    "            article = [para.text for para in divcont.find_all('p')]\n",
    "\n",
    "            article_dict = {'title': title, 'content': article}\n",
    "            article_list.append(article_dict)\n",
    "\n",
    "    codeup_df = pd.DataFrame(article_list)\n",
    "\n",
    "    return codeup_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d213183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_all_articles() missing 1 required positional argument: 'link_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/pxhhvyv90vx8hwbv8yjldhj80000gn/T/ipykernel_63086/3094097994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcodeup_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcodeup_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_all_articles() missing 1 required positional argument: 'link_list'"
     ]
    }
   ],
   "source": [
    "codeup_df = get_all_articles()\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652654b",
   "metadata": {},
   "source": [
    "- Amanda example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f17828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_articles(article_list):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    file = 'blog_posts.json'\n",
    "    \n",
    "    if os.path.exists(file):\n",
    "        with open(file) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    headers = {'User-Agent': 'Codeup Data Science'}\n",
    "    article_info = []\n",
    "    \n",
    "    for article in article_list:\n",
    "        response = get(article, headers=headers)\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        info_dict = {'title':soup.find('h1').text,\n",
    "                    'link':article,\n",
    "                    'date_published':soup.find('spand', class_='published').text,\n",
    "                    'content': soup.find('div', class_='entry-content').text}\n",
    "        article_info.append(info_dict)\n",
    "        \n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(article_info, f)\n",
    "        \n",
    "    return article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_info = get_blog_articles(links_list)\n",
    "article_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef80390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782c6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c75025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1530ede",
   "metadata": {},
   "source": [
    "# 2.\n",
    "News Articles\n",
    "\n",
    "We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "Business\n",
    "Sports\n",
    "Technology\n",
    "Entertainment\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_articles():\n",
    "    links = [\n",
    "        'https://inshorts.com/en/read/sports',\n",
    "        'https://inshorts.com/en/read/business',\n",
    "        'https://inshorts.com/en/read/technology',\n",
    "        'https://inshorts.com/en/read/entertainment'\n",
    "    ]\n",
    "\n",
    "    news_articles = []\n",
    "\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        category = link.split('/')[-1]\n",
    "\n",
    "        articles = soup.find_all('div', class_='news-card')\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.find('span', itemprop='headline').text.strip()\n",
    "            content = article.find('div', itemprop='articleBody').text.strip()\n",
    "\n",
    "            news_articles.append({\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'category': category\n",
    "            })\n",
    "            \n",
    "        articles_df = pd.DataFrame(news_articles)\n",
    "\n",
    "    return articles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982421a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "articles_df = get_news_articles()\n",
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f43a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
