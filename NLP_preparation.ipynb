{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ebec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    # Lowercase everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Replace anything that is not a letter, number, whitespace, or a single quote\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s']\", '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokenize all the words in the string\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def stem(text):\n",
    "    # Apply stemming to all the words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    return stemmed_words\n",
    "\n",
    "def lemmatize(text):\n",
    "    # Apply lemmatization to each word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemmatized_words\n",
    "\n",
    "def remove_stopwords(text, extra_words=[], exclude_words=[]):\n",
    "    # Remove stopwords from the text\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list.extend(extra_words)\n",
    "    stopword_list = [word for word in stopword_list if word not in exclude_words]\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Create the news_df dataframe\n",
    "news_articles = [\n",
    "    {\n",
    "        'title': 'News Article 1',\n",
    "        'content': 'This is the content of news article 1.'\n",
    "    },\n",
    "    {\n",
    "        'title': 'News Article 2',\n",
    "        'content': 'Here is the content of news article 2.'\n",
    "    }\n",
    "]\n",
    "news_df = pd.DataFrame(news_articles)\n",
    "\n",
    "# Apply text cleaning to the news_df dataframe\n",
    "news_df['clean'] = news_df['content'].apply(basic_clean)\n",
    "news_df['clean'] = news_df['clean'].apply(tokenize)\n",
    "news_df['clean'] = news_df['clean'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "news_df['stemmed'] = news_df['clean'].apply(stem)\n",
    "news_df['lemmatized'] = news_df['clean'].apply(lemmatize)\n",
    "\n",
    "# Create the codeup_df dataframe\n",
    "codeup_posts = [\n",
    "    {\n",
    "        'title': 'Codeup Blog Post 1',\n",
    "        'content': 'This is the content of Codeup blog post 1.'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Codeup Blog Post 2',\n",
    "        'content': 'Here is the content of Codeup blog post 2.'\n",
    "    }\n",
    "]\n",
    "codeup_df = pd.DataFrame(codeup_posts)\n",
    "\n",
    "# Apply text cleaning to the codeup_df dataframe\n",
    "codeup_df['clean'] = codeup_df['content'].apply(basic_clean)\n",
    "codeup_df['clean'] = codeup_df['clean'].apply(tokenize)\n",
    "codeup_df['clean'] = codeup_df['clean'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "codeup_df['stemmed'] = codeup_df['clean'].apply(stem)\n",
    "codeup_df['lemmatized'] = codeup_df['clean'].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6ff09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d3910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4af82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb1f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7ac15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7f395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
